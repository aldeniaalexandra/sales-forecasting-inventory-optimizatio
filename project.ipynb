{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc655fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Time series specific imports\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33ffd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Sales Data: (3000888, 6)\n",
      "Stores Info: (54, 5)\n",
      "Oil Prices: (1218, 2)\n",
      "Holidays: (350, 6)\n",
      "\n",
      "Sales Data Columns:\n",
      "['id', 'date', 'store_nbr', 'family', 'sales', 'onpromotion']\n",
      "\n",
      "Sales Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   id           int64  \n",
      " 1   date         object \n",
      " 2   store_nbr    int64  \n",
      " 3   family       object \n",
      " 4   sales        float64\n",
      " 5   onpromotion  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 137.4+ MB\n",
      "None\n",
      "\n",
      "First 5 rows of Sales Data:\n",
      "   id        date  store_nbr      family  sales  onpromotion\n",
      "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1       BOOKS    0.0            0\n",
      "\n",
      "Missing Values Analysis:\n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "family         0\n",
      "sales          0\n",
      "onpromotion    0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "                 id     store_nbr         sales   onpromotion\n",
      "count  3.000888e+06  3.000888e+06  3.000888e+06  3.000888e+06\n",
      "mean   1.500444e+06  2.750000e+01  3.577757e+02  2.602770e+00\n",
      "std    8.662819e+05  1.558579e+01  1.101998e+03  1.221888e+01\n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    7.502218e+05  1.400000e+01  0.000000e+00  0.000000e+00\n",
      "50%    1.500444e+06  2.750000e+01  1.100000e+01  0.000000e+00\n",
      "75%    2.250665e+06  4.100000e+01  1.958473e+02  0.000000e+00\n",
      "max    3.000887e+06  5.400000e+01  1.247170e+05  7.410000e+02\n"
     ]
    }
   ],
   "source": [
    "# Load the main datasets\n",
    "sales_data = pd.read_csv('data/train.csv')\n",
    "stores_info = pd.read_csv('data/stores.csv')\n",
    "oil_prices = pd.read_csv('data/oil.csv')\n",
    "holidays = pd.read_csv('data/holidays_events.csv')\n",
    "\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Sales Data: {sales_data.shape}\")\n",
    "print(f\"Stores Info: {stores_info.shape}\")\n",
    "print(f\"Oil Prices: {oil_prices.shape}\")\n",
    "print(f\"Holidays: {holidays.shape}\")\n",
    "\n",
    "print(\"\\nSales Data Columns:\")\n",
    "print(sales_data.columns.tolist())\n",
    "print(\"\\nSales Data Info:\")\n",
    "print(sales_data.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows of Sales Data:\")\n",
    "print(sales_data.head())\n",
    "\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "print(sales_data.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(sales_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff904be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Total Days: 1687\n",
      "Unique Stores: 54\n",
      "Unique Product Families: 33\n",
      "Missing dates in time series: 4\n",
      "\n",
      "Sales Distribution:\n",
      "Total Sales: $1,073,644,952.20\n",
      "Average Daily Sales per Store-Product: $357.78\n",
      "Zero Sales Records: 939,130\n",
      "Negative Sales (Returns): 0\n",
      "\n",
      "Top 10 Product Families by Sales:\n",
      "family\n",
      "GROCERY I        3.434627e+08\n",
      "BEVERAGES        2.169545e+08\n",
      "PRODUCE          1.227047e+08\n",
      "CLEANING         9.752129e+07\n",
      "DAIRY            6.448771e+07\n",
      "BREAD/BAKERY     4.213395e+07\n",
      "POULTRY          3.187600e+07\n",
      "MEATS            3.108647e+07\n",
      "PERSONAL CARE    2.459205e+07\n",
      "DELI             2.411032e+07\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Top 10 Stores by Sales:\n",
      "store_nbr\n",
      "44    6.208755e+07\n",
      "45    5.449801e+07\n",
      "47    5.094831e+07\n",
      "3     5.048191e+07\n",
      "49    4.342010e+07\n",
      "46    4.189606e+07\n",
      "48    3.593313e+07\n",
      "51    3.291149e+07\n",
      "8     3.049429e+07\n",
      "50    2.865302e+07\n",
      "Name: sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert date column to datetime\n",
    "sales_data['date'] = pd.to_datetime(sales_data['date'])\n",
    "\n",
    "# Basic time series information\n",
    "print(f\"Date Range: {sales_data['date'].min()} to {sales_data['date'].max()}\")\n",
    "print(f\"Total Days: {(sales_data['date'].max() - sales_data['date'].min()).days}\")\n",
    "print(f\"Unique Stores: {sales_data['store_nbr'].nunique()}\")\n",
    "print(f\"Unique Product Families: {sales_data['family'].nunique()}\")\n",
    "\n",
    "# Check for any gaps in time series\n",
    "date_range = pd.date_range(start=sales_data['date'].min(), \n",
    "                          end=sales_data['date'].max(), \n",
    "                          freq='D')\n",
    "missing_dates = set(date_range) - set(sales_data['date'].unique())\n",
    "print(f\"Missing dates in time series: {len(missing_dates)}\")\n",
    "\n",
    "# Sales distribution analysis\n",
    "print(f\"\\nSales Distribution:\")\n",
    "print(f\"Total Sales: ${sales_data['sales'].sum():,.2f}\")\n",
    "print(f\"Average Daily Sales per Store-Product: ${sales_data['sales'].mean():.2f}\")\n",
    "print(f\"Zero Sales Records: {len(sales_data[sales_data['sales'] == 0]):,}\")\n",
    "print(f\"Negative Sales (Returns): {len(sales_data[sales_data['sales'] < 0]):,}\")\n",
    "\n",
    "# Product family analysis\n",
    "print(f\"\\nTop 10 Product Families by Sales:\")\n",
    "family_sales = sales_data.groupby('family')['sales'].sum().sort_values(ascending=False)\n",
    "print(family_sales.head(10))\n",
    "\n",
    "# Store performance preview\n",
    "print(f\"\\nTop 10 Stores by Sales:\")\n",
    "store_sales = sales_data.groupby('store_nbr')['sales'].sum().sort_values(ascending=False)\n",
    "print(store_sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4cbbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Store Information:\n",
      "   store_nbr           city                           state type  cluster\n",
      "0          1          Quito                       Pichincha    D       13\n",
      "1          2          Quito                       Pichincha    D       13\n",
      "2          3          Quito                       Pichincha    D        8\n",
      "3          4          Quito                       Pichincha    D        9\n",
      "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4\n",
      "Store types: type\n",
      "D    18\n",
      "C    15\n",
      "A     9\n",
      "B     8\n",
      "E     4\n",
      "Name: count, dtype: int64\n",
      "Cities: 22\n",
      "States: 16\n",
      "\n",
      "Oil Prices Info:\n",
      "Date range: 2013-01-01 00:00:00 to 2017-08-31 00:00:00\n",
      "Missing oil price values: 43\n",
      "Average oil price: $67.71\n",
      "\n",
      "Holidays Info:\n",
      "Total holidays: 350\n",
      "Holiday types: type\n",
      "Holiday       221\n",
      "Event          56\n",
      "Additional     51\n",
      "Transfer       12\n",
      "Bridge          5\n",
      "Work Day        5\n",
      "Name: count, dtype: int64\n",
      "National holidays: 174\n"
     ]
    }
   ],
   "source": [
    "# Store information analysis\n",
    "print(\"\\nStore Information:\")\n",
    "print(stores_info.head())\n",
    "print(f\"Store types: {stores_info['type'].value_counts()}\")\n",
    "print(f\"Cities: {stores_info['city'].nunique()}\")\n",
    "print(f\"States: {stores_info['state'].nunique()}\")\n",
    "\n",
    "# Oil prices analysis\n",
    "oil_prices['date'] = pd.to_datetime(oil_prices['date'])\n",
    "print(f\"\\nOil Prices Info:\")\n",
    "print(f\"Date range: {oil_prices['date'].min()} to {oil_prices['date'].max()}\")\n",
    "print(f\"Missing oil price values: {oil_prices['dcoilwtico'].isnull().sum()}\")\n",
    "print(f\"Average oil price: ${oil_prices['dcoilwtico'].mean():.2f}\")\n",
    "\n",
    "# Holidays analysis\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "print(f\"\\nHolidays Info:\")\n",
    "print(f\"Total holidays: {len(holidays)}\")\n",
    "print(f\"Holiday types: {holidays['type'].value_counts()}\")\n",
    "print(f\"National holidays: {len(holidays[holidays['locale'] == 'National'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db28f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3,000,888\n",
      "Records with zero sales: 939,130\n",
      "Records with positive sales: 2,061,758\n",
      "\n",
      "Top 5 families for detailed analysis: ['GROCERY I', 'BEVERAGES', 'PRODUCE', 'CLEANING', 'DAIRY']\n",
      "\n",
      "Daily total sales shape: (1684, 1)\n",
      "Date range: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Average daily sales: $637,556.38\n"
     ]
    }
   ],
   "source": [
    "# Create a copy for time series analysis\n",
    "ts_data = sales_data.copy()\n",
    "\n",
    "# Convert date to datetime and set as index\n",
    "ts_data['date'] = pd.to_datetime(ts_data['date'])\n",
    "\n",
    "# Remove zero sales for better forecasting (optional - depends on business context)\n",
    "# For comprehensive analysis, we'll keep zeros but note them\n",
    "print(f\"Total records: {len(ts_data):,}\")\n",
    "print(f\"Records with zero sales: {len(ts_data[ts_data['sales'] == 0]):,}\")\n",
    "print(f\"Records with positive sales: {len(ts_data[ts_data['sales'] > 0]):,}\")\n",
    "\n",
    "# Create aggregated time series at different levels\n",
    "# 1. Daily total sales across all stores and products\n",
    "daily_total_sales = ts_data.groupby('date')['sales'].sum().reset_index()\n",
    "daily_total_sales = daily_total_sales.set_index('date').sort_index()\n",
    "\n",
    "# 2. Daily sales by product family\n",
    "daily_family_sales = ts_data.groupby(['date', 'family'])['sales'].sum().reset_index()\n",
    "\n",
    "# 3. Daily sales by store\n",
    "daily_store_sales = ts_data.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "\n",
    "# 4. Focus on top 5 product families for detailed analysis\n",
    "top_families = family_sales.head(5).index.tolist()\n",
    "print(f\"\\nTop 5 families for detailed analysis: {top_families}\")\n",
    "\n",
    "# Create individual time series for top families\n",
    "family_ts_dict = {}\n",
    "for family in top_families:\n",
    "    family_data = ts_data[ts_data['family'] == family].groupby('date')['sales'].sum()\n",
    "    family_ts_dict[family] = family_data.sort_index()\n",
    "\n",
    "print(f\"\\nDaily total sales shape: {daily_total_sales.shape}\")\n",
    "print(f\"Date range: {daily_total_sales.index.min()} to {daily_total_sales.index.max()}\")\n",
    "print(f\"Average daily sales: ${daily_total_sales['sales'].mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afa81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset shape: (1684, 3)\n",
      "Columns: ['sales', 'dcoilwtico', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "# Merge with oil prices\n",
    "oil_prices_clean = oil_prices.copy()\n",
    "oil_prices_clean['date'] = pd.to_datetime(oil_prices_clean['date'])\n",
    "oil_prices_clean = oil_prices_clean.set_index('date').sort_index()\n",
    "\n",
    "# Forward fill missing oil prices\n",
    "oil_prices_clean['dcoilwtico'] = oil_prices_clean['dcoilwtico'].fillna(method='ffill')\n",
    "oil_prices_clean['dcoilwtico'] = oil_prices_clean['dcoilwtico'].fillna(method='bfill')\n",
    "\n",
    "# Merge oil prices with daily sales\n",
    "daily_sales_enhanced = daily_total_sales.copy()\n",
    "daily_sales_enhanced = daily_sales_enhanced.merge(oil_prices_clean, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Process holidays data\n",
    "holidays_clean = holidays.copy()\n",
    "holidays_clean['date'] = pd.to_datetime(holidays_clean['date'])\n",
    "holidays_clean['is_holiday'] = 1\n",
    "\n",
    "# Create holiday indicator\n",
    "holiday_indicator = holidays_clean[['date', 'is_holiday']].drop_duplicates()\n",
    "holiday_indicator = holiday_indicator.set_index('date')\n",
    "\n",
    "# Merge with daily sales\n",
    "daily_sales_enhanced = daily_sales_enhanced.merge(holiday_indicator, left_index=True, right_index=True, how='left')\n",
    "daily_sales_enhanced['is_holiday'] = daily_sales_enhanced['is_holiday'].fillna(0)\n",
    "\n",
    "print(f\"Enhanced dataset shape: {daily_sales_enhanced.shape}\")\n",
    "print(f\"Columns: {daily_sales_enhanced.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387e3490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete. Dataset shape: (1684, 21)\n",
      "Features created: ['sales', 'dcoilwtico', 'is_holiday', 'year', 'month', 'day_of_week', 'day_of_year', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end', 'sales_lag_1', 'sales_lag_7', 'sales_lag_30', 'sales_lag_365', 'sales_ma_7', 'sales_ma_30', 'sales_ma_90', 'sales_growth_daily', 'sales_growth_weekly', 'sales_growth_monthly']\n"
     ]
    }
   ],
   "source": [
    "# Add time-based features\n",
    "daily_sales_enhanced['year'] = daily_sales_enhanced.index.year\n",
    "daily_sales_enhanced['month'] = daily_sales_enhanced.index.month\n",
    "daily_sales_enhanced['day_of_week'] = daily_sales_enhanced.index.dayofweek\n",
    "daily_sales_enhanced['day_of_year'] = daily_sales_enhanced.index.dayofyear\n",
    "daily_sales_enhanced['quarter'] = daily_sales_enhanced.index.quarter\n",
    "daily_sales_enhanced['is_weekend'] = (daily_sales_enhanced.index.dayofweek >= 5).astype(int)\n",
    "daily_sales_enhanced['is_month_start'] = daily_sales_enhanced.index.is_month_start.astype(int)\n",
    "daily_sales_enhanced['is_month_end'] = daily_sales_enhanced.index.is_month_end.astype(int)\n",
    "\n",
    "# Create lag features\n",
    "daily_sales_enhanced['sales_lag_1'] = daily_sales_enhanced['sales'].shift(1)\n",
    "daily_sales_enhanced['sales_lag_7'] = daily_sales_enhanced['sales'].shift(7)\n",
    "daily_sales_enhanced['sales_lag_30'] = daily_sales_enhanced['sales'].shift(30)\n",
    "daily_sales_enhanced['sales_lag_365'] = daily_sales_enhanced['sales'].shift(365)\n",
    "\n",
    "# Create moving averages\n",
    "daily_sales_enhanced['sales_ma_7'] = daily_sales_enhanced['sales'].rolling(window=7).mean()\n",
    "daily_sales_enhanced['sales_ma_30'] = daily_sales_enhanced['sales'].rolling(window=30).mean()\n",
    "daily_sales_enhanced['sales_ma_90'] = daily_sales_enhanced['sales'].rolling(window=90).mean()\n",
    "\n",
    "# Calculate growth rates\n",
    "daily_sales_enhanced['sales_growth_daily'] = daily_sales_enhanced['sales'].pct_change()\n",
    "daily_sales_enhanced['sales_growth_weekly'] = daily_sales_enhanced['sales'].pct_change(periods=7)\n",
    "daily_sales_enhanced['sales_growth_monthly'] = daily_sales_enhanced['sales'].pct_change(periods=30)\n",
    "\n",
    "print(f\"Feature engineering complete. Dataset shape: {daily_sales_enhanced.shape}\")\n",
    "print(f\"Features created: {daily_sales_enhanced.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ca5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling dataset shape: (942, 21)\n",
      "Date range for modeling: 2014-01-02 00:00:00 to 2017-08-15 00:00:00\n",
      "Training set: 852 days (2014-01-02 00:00:00 to 2017-04-11 00:00:00)\n",
      "Test set: 90 days (2017-04-12 00:00:00 to 2017-08-15 00:00:00)\n",
      "\n",
      "Data preparation complete! Files saved:\n",
      "- daily_sales_enhanced.csv (full dataset with features)\n",
      "- modeling_data.csv (clean dataset for modeling)\n",
      "- train_data.csv (training set)\n",
      "- test_data.csv (test set)\n",
      "- individual family time series files\n",
      "\n",
      "Final Dataset Statistics:\n",
      "Average daily sales: $637,661.98\n",
      "Sales volatility (std): $171,374.52\n",
      "Min daily sales: $12,773.62\n",
      "Max daily sales: $1,402,306.37\n",
      "Holiday days in dataset: 151.0\n",
      "Weekend days: 0\n"
     ]
    }
   ],
   "source": [
    "# Create clean dataset for modeling (remove NaN values created by lags)\n",
    "modeling_data = daily_sales_enhanced.dropna().copy()\n",
    "\n",
    "print(f\"Modeling dataset shape: {modeling_data.shape}\")\n",
    "print(f\"Date range for modeling: {modeling_data.index.min()} to {modeling_data.index.max()}\")\n",
    "\n",
    "# Split data for training and testing\n",
    "# Use last 90 days as test set\n",
    "train_size = len(modeling_data) - 90\n",
    "train_data = modeling_data.iloc[:train_size].copy()\n",
    "test_data = modeling_data.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_data)} days ({train_data.index.min()} to {train_data.index.max()})\")\n",
    "print(f\"Test set: {len(test_data)} days ({test_data.index.min()} to {test_data.index.max()})\")\n",
    "\n",
    "# Save prepared datasets\n",
    "daily_sales_enhanced.to_csv('data/daily_sales_enhanced.csv')\n",
    "modeling_data.to_csv('data/modeling_data.csv')\n",
    "train_data.to_csv('data/train_data.csv')\n",
    "test_data.to_csv('data/test_data.csv')\n",
    "\n",
    "# Save individual family time series\n",
    "for family, ts in family_ts_dict.items():\n",
    "    ts.to_csv(f'data/family_ts_{family.replace(\"/\", \"_\").replace(\" \", \"_\")}.csv')\n",
    "\n",
    "print(\"\\nData preparation complete! Files saved:\")\n",
    "print(\"- daily_sales_enhanced.csv (full dataset with features)\")\n",
    "print(\"- modeling_data.csv (clean dataset for modeling)\")\n",
    "print(\"- train_data.csv (training set)\")\n",
    "print(\"- test_data.csv (test set)\")\n",
    "print(\"- individual family time series files\")\n",
    "\n",
    "# Display final statistics\n",
    "print(f\"\\nFinal Dataset Statistics:\")\n",
    "print(f\"Average daily sales: ${modeling_data['sales'].mean():,.2f}\")\n",
    "print(f\"Sales volatility (std): ${modeling_data['sales'].std():,.2f}\")\n",
    "print(f\"Min daily sales: ${modeling_data['sales'].min():,.2f}\")\n",
    "print(f\"Max daily sales: ${modeling_data['sales'].max():,.2f}\")\n",
    "print(f\"Holiday days in dataset: {modeling_data['is_holiday'].sum()}\")\n",
    "print(f\"Weekend days: {modeling_data['is_weekend'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1e73e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Decomposition Analysis:\n",
      "Original Series - Mean: $637,661.98, Std: $171,374.52\n",
      "Trend Component - Mean: $643,779.34, Std: $77,814.59\n",
      "Seasonal Component - Mean: $7,659.05, Std: $117,426.08\n",
      "Residual Component - Mean: $9,072.70, Std: $77,841.31\n",
      "\n",
      "Seasonality Strength: 0.672\n",
      "Trend Strength: 0.435\n"
     ]
    }
   ],
   "source": [
    "# Load the modeling data\n",
    "ts_analysis = modeling_data['sales'].copy()\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Additive decomposition\n",
    "decomposition_add = seasonal_decompose(ts_analysis, model='additive', period=365)\n",
    "\n",
    "# Multiplicative decomposition\n",
    "decomposition_mult = seasonal_decompose(ts_analysis, model='multiplicative', period=365)\n",
    "\n",
    "# Extract components\n",
    "trend = decomposition_add.trend\n",
    "seasonal = decomposition_add.seasonal\n",
    "residual = decomposition_add.resid\n",
    "\n",
    "print(\"Time Series Decomposition Analysis:\")\n",
    "print(f\"Original Series - Mean: ${ts_analysis.mean():,.2f}, Std: ${ts_analysis.std():,.2f}\")\n",
    "print(f\"Trend Component - Mean: ${trend.mean():,.2f}, Std: ${trend.std():,.2f}\")\n",
    "print(f\"Seasonal Component - Mean: ${seasonal.mean():,.2f}, Std: ${seasonal.std():,.2f}\")\n",
    "print(f\"Residual Component - Mean: ${residual.mean():,.2f}, Std: ${residual.std():,.2f}\")\n",
    "\n",
    "# Calculate seasonality strength\n",
    "seasonal_strength = 1 - (residual.var() / (seasonal + residual).var())\n",
    "trend_strength = 1 - (residual.var() / (trend + residual).var())\n",
    "\n",
    "print(f\"\\nSeasonality Strength: {seasonal_strength:.3f}\")\n",
    "print(f\"Trend Strength: {trend_strength:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0de276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original Sales Series ===\n",
      "ADF Statistic: -2.468688\n",
      "p-value: 0.123280\n",
      "Critical Values:\n",
      "\t1%: -3.437\n",
      "\t5%: -2.865\n",
      "\t10%: -2.568\n",
      "=> Series is non-stationary (fail to reject null hypothesis)\n",
      "\n",
      "KPSS Statistic: 3.360952\n",
      "p-value: 0.010000\n",
      "Critical Values:\n",
      "\t10%: 0.347\n",
      "\t5%: 0.463\n",
      "\t2.5%: 0.574\n",
      "\t1%: 0.739\n",
      "=> Series is non-stationary (reject null hypothesis)\n",
      "\n",
      "=== First Difference ===\n",
      "ADF Statistic: -10.011032\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.437\n",
      "\t5%: -2.865\n",
      "\t10%: -2.568\n",
      "=> Series is stationary (reject null hypothesis)\n",
      "\n",
      "KPSS Statistic: 0.079699\n",
      "p-value: 0.100000\n",
      "Critical Values:\n",
      "\t10%: 0.347\n",
      "\t5%: 0.463\n",
      "\t2.5%: 0.574\n",
      "\t1%: 0.739\n",
      "=> Series is stationary (fail to reject null hypothesis)\n",
      "\n",
      "=== Seasonal Difference (365 days) ===\n",
      "ADF Statistic: -3.678531\n",
      "p-value: 0.004425\n",
      "Critical Values:\n",
      "\t1%: -3.442\n",
      "\t5%: -2.867\n",
      "\t10%: -2.570\n",
      "=> Series is stationary (reject null hypothesis)\n",
      "\n",
      "KPSS Statistic: 0.929166\n",
      "p-value: 0.010000\n",
      "Critical Values:\n",
      "\t10%: 0.347\n",
      "\t5%: 0.463\n",
      "\t2.5%: 0.574\n",
      "\t1%: 0.739\n",
      "=> Series is non-stationary (reject null hypothesis)\n",
      "\n",
      "=== Combined Differencing (1st + Seasonal) ===\n",
      "ADF Statistic: -8.419848\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.442\n",
      "\t5%: -2.867\n",
      "\t10%: -2.570\n",
      "=> Series is stationary (reject null hypothesis)\n",
      "\n",
      "KPSS Statistic: 0.106191\n",
      "p-value: 0.100000\n",
      "Critical Values:\n",
      "\t10%: 0.347\n",
      "\t5%: 0.463\n",
      "\t2.5%: 0.574\n",
      "\t1%: 0.739\n",
      "=> Series is stationary (fail to reject null hypothesis)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "def check_stationarity(timeseries, title):\n",
    "    print(f'\\n=== {title} ===')\n",
    "    \n",
    "    # ADF Test\n",
    "    adf_result = adfuller(timeseries.dropna())\n",
    "    print(f'ADF Statistic: {adf_result[0]:.6f}')\n",
    "    print(f'p-value: {adf_result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"=> Series is stationary (reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"=> Series is non-stationary (fail to reject null hypothesis)\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(timeseries.dropna())\n",
    "    print(f'\\nKPSS Statistic: {kpss_result[0]:.6f}')\n",
    "    print(f'p-value: {kpss_result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "        \n",
    "    if kpss_result[1] <= 0.05:\n",
    "        print(\"=> Series is non-stationary (reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"=> Series is stationary (fail to reject null hypothesis)\")\n",
    "\n",
    "# Test original series\n",
    "check_stationarity(ts_analysis, \"Original Sales Series\")\n",
    "\n",
    "# Create differenced series for stationarity\n",
    "ts_diff1 = ts_analysis.diff().dropna()\n",
    "check_stationarity(ts_diff1, \"First Difference\")\n",
    "\n",
    "# Seasonal differencing\n",
    "ts_seasonal_diff = ts_analysis.diff(365).dropna()\n",
    "check_stationarity(ts_seasonal_diff, \"Seasonal Difference (365 days)\")\n",
    "\n",
    "# Combined differencing\n",
    "ts_combined_diff = ts_analysis.diff().diff(365).dropna()\n",
    "if len(ts_combined_diff) > 0:\n",
    "    check_stationarity(ts_combined_diff, \"Combined Differencing (1st + Seasonal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2611aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seasonal Pattern Analysis ===\n",
      "\n",
      "Monthly Average Sales:\n",
      "Jan: $602,927.47\n",
      "Feb: $571,718.77\n",
      "Mar: $622,936.45\n",
      "Apr: $585,288.48\n",
      "May: $599,640.98\n",
      "Jun: $619,308.00\n",
      "Jul: $666,586.22\n",
      "Aug: $616,904.79\n",
      "Sep: $649,733.31\n",
      "Oct: $639,391.60\n",
      "Nov: $676,609.14\n",
      "Dec: $860,933.94\n",
      "\n",
      "Quarterly Average Sales:\n",
      "Q1: $600,020.44\n",
      "Q2: $601,529.70\n",
      "Q3: $645,333.69\n",
      "Q4: $726,399.29\n",
      "\n",
      "Weekly Pattern (0=Monday, 6=Sunday):\n",
      "Mon: $683,879.62\n",
      "Tue: $636,681.52\n",
      "Wed: $664,341.99\n",
      "Thu: $559,465.90\n",
      "Fri: $643,700.26\n",
      "\n",
      "Holiday Effect Analysis:\n",
      "Non-holiday days: Mean=$622,436.95, Std=$154,162.27, Count=791\n",
      "Holiday days: Mean=$717,416.96, Std=$226,825.25, Count=151\n",
      "Holiday Impact: +15.26%\n"
     ]
    }
   ],
   "source": [
    "# Analyze seasonal patterns\n",
    "monthly_avg = modeling_data.groupby('month')['sales'].mean()\n",
    "quarterly_avg = modeling_data.groupby('quarter')['sales'].mean()\n",
    "weekly_avg = modeling_data.groupby('day_of_week')['sales'].mean()\n",
    "\n",
    "print(f\"\\n=== Seasonal Pattern Analysis ===\")\n",
    "print(f\"\\nMonthly Average Sales:\")\n",
    "for month, sales in monthly_avg.items():\n",
    "    month_names = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n",
    "                  7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    print(f\"{month_names[month]}: ${sales:,.2f}\")\n",
    "\n",
    "print(f\"\\nQuarterly Average Sales:\")\n",
    "for quarter, sales in quarterly_avg.items():\n",
    "    print(f\"Q{quarter}: ${sales:,.2f}\")\n",
    "\n",
    "print(f\"\\nWeekly Pattern (0=Monday, 6=Sunday):\")\n",
    "day_names = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "for day, sales in weekly_avg.items():\n",
    "    print(f\"{day_names[day]}: ${sales:,.2f}\")\n",
    "\n",
    "# Holiday effect analysis\n",
    "holiday_effect = modeling_data.groupby('is_holiday')['sales'].agg(['mean', 'std', 'count'])\n",
    "print(f\"\\nHoliday Effect Analysis:\")\n",
    "print(f\"Non-holiday days: Mean=${holiday_effect.loc[0, 'mean']:,.2f}, Std=${holiday_effect.loc[0, 'std']:,.2f}, Count={holiday_effect.loc[0, 'count']}\")\n",
    "print(f\"Holiday days: Mean=${holiday_effect.loc[1, 'mean']:,.2f}, Std=${holiday_effect.loc[1, 'std']:,.2f}, Count={holiday_effect.loc[1, 'count']}\")\n",
    "\n",
    "holiday_impact = ((holiday_effect.loc[1, 'mean'] - holiday_effect.loc[0, 'mean']) / holiday_effect.loc[0, 'mean']) * 100\n",
    "print(f\"Holiday Impact: {holiday_impact:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7998d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Correlation Analysis ===\n",
      "Correlation with Sales:\n",
      "sales_lag_1: 0.700\n",
      "sales_lag_7: 0.662\n",
      "sales_lag_30: 0.536\n",
      "dcoilwtico: -0.516\n",
      "is_holiday: 0.203\n",
      "is_weekend: nan\n",
      "\n",
      "Oil Price Impact on Sales:\n",
      "Low Oil Prices: $699,648.18\n",
      "Med-Low Oil Prices: $681,295.20\n",
      "Med-High Oil Prices: $677,931.35\n",
      "High Oil Prices: $492,128.72\n",
      "\n",
      "Year-over-Year Performance:\n",
      "2014: Mean=$516,973.56, Growth=+0.0%\n",
      "2015: Mean=$594,083.91, Growth=+14.9%\n",
      "2016: Mean=$713,996.63, Growth=+38.1%\n",
      "2017: Mean=$777,570.98, Growth=+50.4%\n",
      "\n",
      "Analysis results saved:\n",
      "- time_series_decomposition.csv\n",
      "- correlation_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Correlation analysis\n",
    "correlation_features = ['sales', 'dcoilwtico', 'is_holiday', 'is_weekend', \n",
    "                       'sales_lag_1', 'sales_lag_7', 'sales_lag_30']\n",
    "\n",
    "correlation_matrix = modeling_data[correlation_features].corr()\n",
    "\n",
    "print(f\"\\n=== Correlation Analysis ===\")\n",
    "print(f\"Correlation with Sales:\")\n",
    "sales_correlations = correlation_matrix['sales'].sort_values(key=abs, ascending=False)\n",
    "for feature, corr in sales_correlations.items():\n",
    "    if feature != 'sales':\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Oil price impact analysis\n",
    "oil_price_bins = pd.qcut(modeling_data['dcoilwtico'], q=4, labels=['Low', 'Med-Low', 'Med-High', 'High'])\n",
    "oil_impact = modeling_data.groupby(oil_price_bins)['sales'].mean()\n",
    "\n",
    "print(f\"\\nOil Price Impact on Sales:\")\n",
    "for price_level, sales in oil_impact.items():\n",
    "    print(f\"{price_level} Oil Prices: ${sales:,.2f}\")\n",
    "\n",
    "# Year-over-year analysis\n",
    "yearly_stats = modeling_data.groupby('year')['sales'].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "print(f\"\\nYear-over-Year Performance:\")\n",
    "for year, stats in yearly_stats.iterrows():\n",
    "    print(f\"{year}: Mean=${stats['mean']:,.2f}, Growth={(stats['mean']/yearly_stats.iloc[0]['mean']-1)*100:+.1f}%\")\n",
    "\n",
    "# Save analysis results\n",
    "decomposition_results = pd.DataFrame({\n",
    "    'original': ts_analysis,\n",
    "    'trend': trend,\n",
    "    'seasonal': seasonal,\n",
    "    'residual': residual\n",
    "})\n",
    "\n",
    "seasonality_analysis = pd.DataFrame({\n",
    "    'monthly_avg': monthly_avg,\n",
    "    'quarterly_avg': monthly_avg.index.map(lambda m: quarterly_avg.get(m // 4 + 1)),\n",
    "    'weekly_pattern': monthly_avg.index.map(lambda m: weekly_avg.get(modeling_data.loc[modeling_data['month'] == m, 'day_of_week'].mode()[0]))\n",
    "})\n",
    "\n",
    "# Save results\n",
    "decomposition_results.to_csv('output/time_series_decomposition.csv')\n",
    "correlation_matrix.to_csv('output/correlation_analysis.csv')\n",
    "\n",
    "print(f\"\\nAnalysis results saved:\")\n",
    "print(\"- time_series_decomposition.csv\")\n",
    "print(\"- correlation_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce916e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ARIMA Model Development ===\n",
      "Finding optimal ARIMA parameters...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,0,0)[30] intercept   : AIC=22218.349, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,0,0)[30] intercept   : AIC=22057.969, Time=0.63 sec\n",
      " ARIMA(0,1,1)(0,0,1)[30] intercept   : AIC=22023.348, Time=0.58 sec\n",
      " ARIMA(0,1,0)(0,0,0)[30]             : AIC=22216.350, Time=0.01 sec\n",
      " ARIMA(0,1,1)(0,0,0)[30] intercept   : AIC=22056.446, Time=0.07 sec\n",
      " ARIMA(0,1,1)(1,0,1)[30] intercept   : AIC=22001.257, Time=0.98 sec\n",
      " ARIMA(0,1,1)(1,0,0)[30] intercept   : AIC=22015.382, Time=0.61 sec\n",
      " ARIMA(0,1,0)(1,0,1)[30] intercept   : AIC=22120.865, Time=0.74 sec\n",
      " ARIMA(1,1,1)(1,0,1)[30] intercept   : AIC=21972.478, Time=1.50 sec\n",
      " ARIMA(1,1,1)(0,0,1)[30] intercept   : AIC=21995.980, Time=1.07 sec\n",
      " ARIMA(1,1,1)(1,0,0)[30] intercept   : AIC=21987.558, Time=0.95 sec\n",
      " ARIMA(1,1,1)(0,0,0)[30] intercept   : AIC=22025.980, Time=0.15 sec\n",
      " ARIMA(1,1,0)(1,0,1)[30] intercept   : AIC=22038.242, Time=1.19 sec\n",
      " ARIMA(2,1,1)(1,0,1)[30] intercept   : AIC=21969.330, Time=2.57 sec\n",
      " ARIMA(2,1,1)(0,0,1)[30] intercept   : AIC=21990.207, Time=2.16 sec\n",
      " ARIMA(2,1,1)(1,0,0)[30] intercept   : AIC=21981.656, Time=1.62 sec\n",
      " ARIMA(2,1,1)(0,0,0)[30] intercept   : AIC=22020.803, Time=0.23 sec\n",
      " ARIMA(2,1,0)(1,0,1)[30] intercept   : AIC=22012.099, Time=1.50 sec\n",
      " ARIMA(3,1,1)(1,0,1)[30] intercept   : AIC=21970.339, Time=3.22 sec\n",
      " ARIMA(2,1,2)(1,0,1)[30] intercept   : AIC=21970.132, Time=3.71 sec\n",
      " ARIMA(1,1,2)(1,0,1)[30] intercept   : AIC=21987.438, Time=3.33 sec\n",
      " ARIMA(3,1,0)(1,0,1)[30] intercept   : AIC=22008.550, Time=1.53 sec\n",
      " ARIMA(3,1,2)(1,0,1)[30] intercept   : AIC=21958.958, Time=3.46 sec\n",
      " ARIMA(3,1,2)(0,0,1)[30] intercept   : AIC=21975.009, Time=2.54 sec\n",
      " ARIMA(3,1,2)(1,0,0)[30] intercept   : AIC=21968.313, Time=2.41 sec\n",
      " ARIMA(3,1,2)(0,0,0)[30] intercept   : AIC=22003.236, Time=0.36 sec\n",
      " ARIMA(3,1,3)(1,0,1)[30] intercept   : AIC=21906.392, Time=4.95 sec\n",
      " ARIMA(3,1,3)(0,0,1)[30] intercept   : AIC=21908.092, Time=5.04 sec\n",
      " ARIMA(3,1,3)(1,0,0)[30] intercept   : AIC=21907.484, Time=4.92 sec\n",
      " ARIMA(3,1,3)(0,0,0)[30] intercept   : AIC=21911.515, Time=0.72 sec\n",
      " ARIMA(2,1,3)(1,0,1)[30] intercept   : AIC=21964.636, Time=5.44 sec\n",
      " ARIMA(3,1,3)(1,0,1)[30]             : AIC=21902.377, Time=6.65 sec\n",
      " ARIMA(3,1,3)(0,0,1)[30]             : AIC=21906.365, Time=4.08 sec\n",
      " ARIMA(3,1,3)(1,0,0)[30]             : AIC=21905.717, Time=4.78 sec\n",
      " ARIMA(3,1,3)(0,0,0)[30]             : AIC=21909.868, Time=0.72 sec\n",
      " ARIMA(2,1,3)(1,0,1)[30]             : AIC=21953.590, Time=12.39 sec\n",
      " ARIMA(3,1,2)(1,0,1)[30]             : AIC=21949.545, Time=2.15 sec\n",
      " ARIMA(2,1,2)(1,0,1)[30]             : AIC=21967.132, Time=1.91 sec\n",
      "\n",
      "Best model:  ARIMA(3,1,3)(1,0,1)[30]          \n",
      "Total fit time: 90.999 seconds\n",
      "Optimal ARIMA model: (3, 1, 3) x (1, 0, 1, 30)\n",
      "\n",
      "ARIMA Model Performance:\n",
      "MAE: $57,255.57\n",
      "RMSE: $87,800.40\n",
      "MAPE: 7.06%\n"
     ]
    }
   ],
   "source": [
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Prepare training and test data\n",
    "train_ts = train_data['sales']\n",
    "test_ts = test_data['sales']\n",
    "\n",
    "print(\"=== ARIMA Model Development ===\")\n",
    "\n",
    "# Auto ARIMA to find optimal parameters\n",
    "print(\"Finding optimal ARIMA parameters...\")\n",
    "auto_arima_model = auto_arima(train_ts, \n",
    "                             start_p=0, start_q=0, \n",
    "                             max_p=3, max_q=3, \n",
    "                             seasonal=True, \n",
    "                             start_P=0, start_Q=0, \n",
    "                             max_P=1, max_Q=1, \n",
    "                             m=30,  # seasonal period\n",
    "                             stepwise=True,\n",
    "                             suppress_warnings=True,\n",
    "                             error_action='ignore',\n",
    "                             trace=True)\n",
    "\n",
    "print(f\"Optimal ARIMA model: {auto_arima_model.order} x {auto_arima_model.seasonal_order}\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(train_ts, order=auto_arima_model.order, \n",
    "                   seasonal_order=auto_arima_model.seasonal_order)\n",
    "arima_fitted = arima_model.fit()\n",
    "\n",
    "# Generate forecasts\n",
    "arima_forecast = arima_fitted.forecast(steps=len(test_ts))\n",
    "arima_forecast_ci = arima_fitted.get_forecast(steps=len(test_ts)).conf_int()\n",
    "\n",
    "# Calculate metrics\n",
    "arima_mae = mean_absolute_error(test_ts, arima_forecast)\n",
    "arima_rmse = np.sqrt(mean_squared_error(test_ts, arima_forecast))\n",
    "arima_mape = mean_absolute_percentage_error(test_ts, arima_forecast) * 100\n",
    "\n",
    "print(f\"\\nARIMA Model Performance:\")\n",
    "print(f\"MAE: ${arima_mae:,.2f}\")\n",
    "print(f\"RMSE: ${arima_rmse:,.2f}\")\n",
    "print(f\"MAPE: {arima_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84271fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prophet Model Development ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:52:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:56 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet Model Performance:\n",
      "MAE: $684,828.72\n",
      "RMSE: $1,135,093.39\n",
      "MAPE: 88.54%\n"
     ]
    }
   ],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "print(\"\\n=== Prophet Model Development ===\")\n",
    "\n",
    "# Prepare data for Prophet\n",
    "prophet_train = train_data.reset_index()[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "prophet_test = test_data.reset_index()[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "\n",
    "# Add external regressors\n",
    "prophet_train_enhanced = prophet_train.copy()\n",
    "prophet_train_enhanced['oil_price'] = train_data['dcoilwtico'].values\n",
    "prophet_train_enhanced['is_holiday'] = train_data['is_holiday'].values\n",
    "\n",
    "# Initialize Prophet model with seasonality\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='multiplicative',\n",
    "    interval_width=0.95,\n",
    "    changepoint_prior_scale=0.05\n",
    ")\n",
    "\n",
    "# Add external regressors\n",
    "prophet_model.add_regressor('oil_price')\n",
    "prophet_model.add_regressor('is_holiday')\n",
    "\n",
    "# Fit model\n",
    "prophet_model.fit(prophet_train_enhanced)\n",
    "\n",
    "# Create future dataframe\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_ts), freq='D')\n",
    "\n",
    "# Add regressor values for future dates\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "future_enhanced = future.merge(\n",
    "    all_data[['dcoilwtico', 'is_holiday']].reset_index().rename(columns={'date': 'ds'}),\n",
    "    on='ds', how='left'\n",
    ")\n",
    "future_enhanced = future_enhanced.rename(columns={'dcoilwtico': 'oil_price'})\n",
    "future_enhanced['oil_price'] = future_enhanced['oil_price'].fillna(method='ffill')\n",
    "future_enhanced['is_holiday'] = future_enhanced['is_holiday'].fillna(0)\n",
    "\n",
    "# Generate forecast\n",
    "prophet_forecast = prophet_model.predict(future_enhanced)\n",
    "\n",
    "# Extract test period forecasts\n",
    "prophet_test_forecast = prophet_forecast.iloc[-len(test_ts):]['yhat'].values\n",
    "\n",
    "# Calculate metrics\n",
    "prophet_mae = mean_absolute_error(test_ts, prophet_test_forecast)\n",
    "prophet_rmse = np.sqrt(mean_squared_error(test_ts, prophet_test_forecast))\n",
    "prophet_mape = mean_absolute_percentage_error(test_ts, prophet_test_forecast) * 100\n",
    "\n",
    "print(f\"Prophet Model Performance:\")\n",
    "print(f\"MAE: ${prophet_mae:,.2f}\")\n",
    "print(f\"RMSE: ${prophet_rmse:,.2f}\")\n",
    "print(f\"MAPE: {prophet_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3628bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exponential Smoothing Model Development ===\n",
      "Exponential Smoothing Model Performance:\n",
      "MAE: $121,493.22\n",
      "RMSE: $178,009.02\n",
      "MAPE: 15.83%\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "print(\"\\n=== Exponential Smoothing Model Development ===\")\n",
    "\n",
    "# Fit Holt-Winters model\n",
    "exp_smooth_model = ExponentialSmoothing(\n",
    "    train_ts,\n",
    "    trend='add',\n",
    "    seasonal='add',\n",
    "    seasonal_periods=365,\n",
    "    damped_trend=True\n",
    ")\n",
    "\n",
    "exp_smooth_fitted = exp_smooth_model.fit(optimized=True)\n",
    "\n",
    "# Generate forecasts\n",
    "exp_smooth_forecast = exp_smooth_fitted.forecast(steps=len(test_ts))\n",
    "\n",
    "# Calculate metrics\n",
    "exp_smooth_mae = mean_absolute_error(test_ts, exp_smooth_forecast)\n",
    "exp_smooth_rmse = np.sqrt(mean_squared_error(test_ts, exp_smooth_forecast))\n",
    "exp_smooth_mape = mean_absolute_percentage_error(test_ts, exp_smooth_forecast) * 100\n",
    "\n",
    "print(f\"Exponential Smoothing Model Performance:\")\n",
    "print(f\"MAE: ${exp_smooth_mae:,.2f}\")\n",
    "print(f\"RMSE: ${exp_smooth_rmse:,.2f}\")\n",
    "print(f\"MAPE: {exp_smooth_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99b0465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear Regression Model Development ===\n",
      "Linear Regression Model Performance:\n",
      "MAE: $58,153.23\n",
      "RMSE: $77,780.61\n",
      "MAPE: 7.52%\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "           feature    coefficient\n",
      "12      sales_ma_7  136299.274469\n",
      "3            month   38301.849931\n",
      "5          quarter  -30390.555033\n",
      "7   is_month_start   16529.803717\n",
      "9      sales_lag_1   15434.808879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n=== Linear Regression Model Development ===\")\n",
    "\n",
    "# Select features for regression\n",
    "feature_columns = ['dcoilwtico', 'is_holiday', 'year', 'month', 'day_of_week', \n",
    "                  'quarter', 'is_weekend', 'is_month_start', 'is_month_end',\n",
    "                  'sales_lag_1', 'sales_lag_7', 'sales_lag_30', 'sales_ma_7', 'sales_ma_30']\n",
    "\n",
    "X_train = train_data[feature_columns].fillna(method='ffill')\n",
    "y_train = train_data['sales']\n",
    "X_test = test_data[feature_columns].fillna(method='ffill')\n",
    "y_test = test_data['sales']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Generate forecasts\n",
    "lr_forecast = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_mae = mean_absolute_error(y_test, lr_forecast)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_forecast))\n",
    "lr_mape = mean_absolute_percentage_error(y_test, lr_forecast) * 100\n",
    "\n",
    "print(f\"Linear Regression Model Performance:\")\n",
    "print(f\"MAE: ${lr_mae:,.2f}\")\n",
    "print(f\"RMSE: ${lr_rmse:,.2f}\")\n",
    "print(f\"MAPE: {lr_mape:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'coefficient': lr_model.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69965583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "                   Model            MAE          RMSE       MAPE\n",
      "0                  ARIMA   57255.572431  8.780040e+04   7.060908\n",
      "1      Linear Regression   58153.226039  7.778061e+04   7.519752\n",
      "2  Exponential Smoothing  121493.222610  1.780090e+05  15.825601\n",
      "3                Prophet  684828.721449  1.135093e+06  88.543934\n",
      "\n",
      "Ensemble Model Performance:\n",
      "MAE: $196,952.46\n",
      "RMSE: $293,744.74\n",
      "MAPE: 25.15%\n",
      "\n",
      "Forecasting complete! Results saved:\n",
      "- forecast_results.csv (all model predictions)\n",
      "- model_comparison.csv (model performance metrics)\n",
      "\n",
      "Best performing model: ARIMA (MAPE: 7.06%)\n"
     ]
    }
   ],
   "source": [
    "# Create comparison dataframe\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'Prophet', 'Exponential Smoothing', 'Linear Regression'],\n",
    "    'MAE': [arima_mae, prophet_mae, exp_smooth_mae, lr_mae],\n",
    "    'RMSE': [arima_rmse, prophet_rmse, exp_smooth_rmse, lr_rmse],\n",
    "    'MAPE': [arima_mape, prophet_mape, exp_smooth_mape, lr_mape]\n",
    "})\n",
    "\n",
    "model_comparison = model_comparison.sort_values('MAPE').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Create ensemble forecast (simple average)\n",
    "ensemble_forecast = (arima_forecast + prophet_test_forecast + exp_smooth_forecast + lr_forecast) / 4\n",
    "\n",
    "ensemble_mae = mean_absolute_error(test_ts, ensemble_forecast)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(test_ts, ensemble_forecast))\n",
    "ensemble_mape = mean_absolute_percentage_error(test_ts, ensemble_forecast) * 100\n",
    "\n",
    "print(f\"\\nEnsemble Model Performance:\")\n",
    "print(f\"MAE: ${ensemble_mae:,.2f}\")\n",
    "print(f\"RMSE: ${ensemble_rmse:,.2f}\")\n",
    "print(f\"MAPE: {ensemble_mape:.2f}%\")\n",
    "\n",
    "# Save forecasting results\n",
    "forecast_results = pd.DataFrame({\n",
    "    'date': test_data.index,\n",
    "    'actual': test_ts.values,\n",
    "    'arima': arima_forecast,\n",
    "    'prophet': prophet_test_forecast,\n",
    "    'exp_smoothing': exp_smooth_forecast,\n",
    "    'linear_regression': lr_forecast,\n",
    "    'ensemble': ensemble_forecast\n",
    "})\n",
    "\n",
    "forecast_results.to_csv('output/forecast_results.csv', index=False)\n",
    "model_comparison_with_ensemble = pd.concat([\n",
    "    model_comparison,\n",
    "    pd.DataFrame({'Model': ['Ensemble'], 'MAE': [ensemble_mae], 'RMSE': [ensemble_rmse], 'MAPE': [ensemble_mape]})\n",
    "])\n",
    "model_comparison_with_ensemble.to_csv('output/model_comparison.csv', index=False)\n",
    "\n",
    "print(f\"\\nForecasting complete! Results saved:\")\n",
    "print(\"- forecast_results.csv (all model predictions)\")\n",
    "print(\"- model_comparison.csv (model performance metrics)\")\n",
    "print(f\"\\nBest performing model: {model_comparison.iloc[0]['Model']} (MAPE: {model_comparison.iloc[0]['MAPE']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d457d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Google Data Studio Dataset Preparation ===\n",
      "Main forecasting dataset: (90, 22)\n",
      "Model performance dataset: (5, 8)\n",
      "Historical enhanced dataset: (942, 29)\n"
     ]
    }
   ],
   "source": [
    "# Load forecast results\n",
    "forecast_df = pd.read_csv('output/forecast_results.csv')\n",
    "forecast_df['date'] = pd.to_datetime(forecast_df['date'])\n",
    "\n",
    "print(\"=== Google Data Studio Dataset Preparation ===\")\n",
    "\n",
    "# 1. Main forecasting dashboard dataset\n",
    "gds_main = forecast_df.copy()\n",
    "\n",
    "# Add performance metrics for each model\n",
    "gds_main['arima_error'] = abs(gds_main['actual'] - gds_main['arima'])\n",
    "gds_main['prophet_error'] = abs(gds_main['actual'] - gds_main['prophet'])\n",
    "gds_main['exp_smoothing_error'] = abs(gds_main['actual'] - gds_main['exp_smoothing'])\n",
    "gds_main['linear_regression_error'] = abs(gds_main['actual'] - gds_main['linear_regression'])\n",
    "gds_main['ensemble_error'] = abs(gds_main['actual'] - gds_main['ensemble'])\n",
    "\n",
    "# Add percentage errors\n",
    "gds_main['arima_pct_error'] = (gds_main['arima_error'] / gds_main['actual']) * 100\n",
    "gds_main['prophet_pct_error'] = (gds_main['prophet_error'] / gds_main['actual']) * 100\n",
    "gds_main['exp_smoothing_pct_error'] = (gds_main['exp_smoothing_error'] / gds_main['actual']) * 100\n",
    "gds_main['linear_regression_pct_error'] = (gds_main['linear_regression_error'] / gds_main['actual']) * 100\n",
    "gds_main['ensemble_pct_error'] = (gds_main['ensemble_error'] / gds_main['actual']) * 100\n",
    "\n",
    "# Add time features for filtering\n",
    "gds_main['year'] = gds_main['date'].dt.year\n",
    "gds_main['month'] = gds_main['date'].dt.month\n",
    "gds_main['month_name'] = gds_main['date'].dt.strftime('%B')\n",
    "gds_main['week'] = gds_main['date'].dt.isocalendar().week\n",
    "gds_main['day_of_week'] = gds_main['date'].dt.day_name()\n",
    "\n",
    "print(f\"Main forecasting dataset: {gds_main.shape}\")\n",
    "\n",
    "# 2. Model performance summary for KPI cards\n",
    "model_performance = pd.read_csv('output/model_comparison.csv')\n",
    "\n",
    "# Add ranking\n",
    "model_performance['rank_mae'] = model_performance['MAE'].rank()\n",
    "model_performance['rank_rmse'] = model_performance['RMSE'].rank()\n",
    "model_performance['rank_mape'] = model_performance['MAPE'].rank()\n",
    "model_performance['overall_rank'] = (model_performance['rank_mae'] + \n",
    "                                    model_performance['rank_rmse'] + \n",
    "                                    model_performance['rank_mape']) / 3\n",
    "\n",
    "model_performance = model_performance.sort_values('overall_rank').reset_index(drop=True)\n",
    "\n",
    "print(f\"Model performance dataset: {model_performance.shape}\")\n",
    "\n",
    "# 3. Time series historical data with decomposition\n",
    "historical_data['date'] = pd.to_datetime(historical_data['date'])\n",
    "\n",
    "# Add decomposition components\n",
    "decomposition_df['date'] = pd.to_datetime(decomposition_df['date'])\n",
    "historical_enhanced = historical_data.merge(decomposition_df, on='date', how='left')\n",
    "\n",
    "# Add year-over-year growth\n",
    "historical_enhanced['sales_yoy_growth'] = historical_enhanced.groupby(\n",
    "    [historical_enhanced['date'].dt.month, historical_enhanced['date'].dt.day]\n",
    ")['sales'].pct_change(periods=365) * 100\n",
    "\n",
    "# Add moving averages for trend analysis\n",
    "historical_enhanced['sales_trend_30d'] = historical_enhanced['sales'].rolling(30).mean()\n",
    "historical_enhanced['sales_trend_90d'] = historical_enhanced['sales'].rolling(90).mean()\n",
    "\n",
    "print(f\"Historical enhanced dataset: {historical_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13766388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal insights dataset: (44, 12)\n",
      "Weekly accuracy dataset: (19, 7)\n",
      "Future forecast dataset: (30, 7)\n"
     ]
    }
   ],
   "source": [
    "# 4. Seasonal insights for business planning\n",
    "seasonal_insights = modeling_data.groupby(['month', 'year']).agg({\n",
    "    'sales': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "    'is_holiday': 'sum',\n",
    "    'dcoilwtico': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "seasonal_insights.columns = [f\"{col[1]}_{col[0]}\" if col[1] else col[0] for col in seasonal_insights.columns]\n",
    "seasonal_insights = seasonal_insights.reset_index()\n",
    "\n",
    "# Add month names for better visualization\n",
    "seasonal_insights['month_name'] = pd.to_datetime(seasonal_insights['month'], format='%m').dt.strftime('%B')\n",
    "\n",
    "# Calculate seasonal factors\n",
    "overall_mean = modeling_data['sales'].mean()\n",
    "seasonal_insights['seasonal_factor'] = seasonal_insights['mean_sales'] / overall_mean\n",
    "seasonal_insights['seasonality_category'] = pd.cut(\n",
    "    seasonal_insights['seasonal_factor'], \n",
    "    bins=[0, 0.9, 1.1, float('inf')], \n",
    "    labels=['Below Average', 'Average', 'Above Average']\n",
    ")\n",
    "\n",
    "print(f\"Seasonal insights dataset: {seasonal_insights.shape}\")\n",
    "\n",
    "# 5. Accuracy metrics by time period\n",
    "gds_main['forecast_period'] = 'Test Period (Apr-Aug 2017)'\n",
    "\n",
    "weekly_accuracy = gds_main.groupby('week').agg({\n",
    "    'arima_pct_error': 'mean',\n",
    "    'linear_regression_pct_error': 'mean',\n",
    "    'exp_smoothing_pct_error': 'mean',\n",
    "    'ensemble_pct_error': 'mean',\n",
    "    'actual': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "weekly_accuracy['best_model'] = weekly_accuracy[['arima_pct_error', 'linear_regression_pct_error', \n",
    "                                                'exp_smoothing_pct_error', 'ensemble_pct_error']].idxmin(axis=1)\n",
    "weekly_accuracy = weekly_accuracy.reset_index()\n",
    "\n",
    "print(f\"Weekly accuracy dataset: {weekly_accuracy.shape}\")\n",
    "\n",
    "# 6. Future forecast scenario (next 30 days)\n",
    "# Generate future forecasts using best model (ARIMA)\n",
    "last_date = modeling_data.index.max()\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "\n",
    "# Simple extrapolation for demonstration (in real scenario, retrain model)\n",
    "future_trend = modeling_data['sales'].tail(30).mean()\n",
    "seasonal_pattern = modeling_data.groupby(modeling_data.index.dayofyear)['sales'].mean()\n",
    "\n",
    "future_forecasts = []\n",
    "for date in future_dates:\n",
    "    day_of_year = date.dayofyear\n",
    "    if day_of_year in seasonal_pattern.index:\n",
    "        seasonal_adj = seasonal_pattern[day_of_year] / modeling_data['sales'].mean()\n",
    "    else:\n",
    "        seasonal_adj = 1.0\n",
    "    \n",
    "    forecast_value = future_trend * seasonal_adj\n",
    "    future_forecasts.append(forecast_value)\n",
    "\n",
    "future_forecast_df = pd.DataFrame({\n",
    "    'date': future_dates,\n",
    "    'forecast_sales': future_forecasts,\n",
    "    'forecast_type': 'ARIMA_Projection',\n",
    "    'confidence_level': 'Medium'\n",
    "})\n",
    "\n",
    "future_forecast_df['year'] = future_forecast_df['date'].dt.year\n",
    "future_forecast_df['month'] = future_forecast_df['date'].dt.month\n",
    "future_forecast_df['month_name'] = future_forecast_df['date'].dt.strftime('%B')\n",
    "\n",
    "print(f\"Future forecast dataset: {future_forecast_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9850657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google Data Studio datasets exported successfully:\n",
      "- gds_main_forecasting.csv (forecast comparisons)\n",
      "- gds_model_performance.csv (model metrics)\n",
      "- gds_historical_analysis.csv (historical data)\n",
      "- gds_seasonal_insights.csv (seasonal patterns)\n",
      "- gds_weekly_accuracy.csv (accuracy analysis)\n",
      "- gds_future_forecasts.csv (future projections)\n",
      "- gds_business_summary.csv (executive KPIs)\n",
      "- gds_setup_guide.txt (dashboard setup guide)\n",
      "\n",
      "Key Insights for Dashboard:\n",
      "• Best Model: ARIMA with 7.06% average error\n",
      "• Total Forecast Period: 90 days\n",
      "• Average Daily Sales: $776,120\n",
      "• Seasonal Peak: December ($781,552 avg)\n",
      "• Future Revenue Projection: $22,826,852 (next 30 days)\n"
     ]
    }
   ],
   "source": [
    "# Export datasets optimized for Google Data Studio\n",
    "gds_main.to_csv('output/gds_main_forecasting.csv', index=False)\n",
    "model_performance.to_csv('output/gds_model_performance.csv', index=False)\n",
    "historical_enhanced.to_csv('output/gds_historical_analysis.csv', index=False)\n",
    "seasonal_insights.to_csv('output/gds_seasonal_insights.csv', index=False)\n",
    "weekly_accuracy.to_csv('output/gds_weekly_accuracy.csv', index=False)\n",
    "future_forecast_df.to_csv('output/gds_future_forecasts.csv', index=False)\n",
    "\n",
    "# Create business summary for executives\n",
    "business_summary = {\n",
    "    'total_actual_sales': gds_main['actual'].sum(),\n",
    "    'total_forecast_sales': gds_main['arima'].sum(),\n",
    "    'forecast_accuracy': f\"{100 - gds_main['arima_pct_error'].mean():.1f}%\",\n",
    "    'best_model': 'ARIMA',\n",
    "    'worst_model': 'Prophet',\n",
    "    'forecast_period_days': len(gds_main),\n",
    "    'average_daily_sales': gds_main['actual'].mean(),\n",
    "    'peak_sales_day': gds_main.loc[gds_main['actual'].idxmax(), 'date'].strftime('%Y-%m-%d'),\n",
    "    'lowest_sales_day': gds_main.loc[gds_main['actual'].idxmin(), 'date'].strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "business_summary_df = pd.DataFrame([business_summary])\n",
    "business_summary_df.to_csv('output/gds_business_summary.csv', index=False)\n",
    "\n",
    "# Create Google Data Studio connection guide\n",
    "gds_guide = \"\"\"\n",
    "# Google Data Studio Setup Guide - Sales Forecasting Dashboard\n",
    "\n",
    "## Data Sources Required:\n",
    "1. gds_main_forecasting.csv - Main forecast comparisons\n",
    "2. gds_model_performance.csv - Model accuracy metrics  \n",
    "3. gds_historical_analysis.csv - Historical trends and patterns\n",
    "4. gds_seasonal_insights.csv - Seasonal business patterns\n",
    "5. gds_weekly_accuracy.csv - Accuracy by time period\n",
    "6. gds_future_forecasts.csv - Next 30 days projections\n",
    "7. gds_business_summary.csv - Executive KPI summary\n",
    "\n",
    "## Recommended Dashboard Pages:\n",
    "\n",
    "### Page 1: Executive Summary\n",
    "- KPI cards: Total Sales, Forecast Accuracy, Best Model, Future Revenue\n",
    "- Line chart: Actual vs Predicted (last 90 days)\n",
    "- Bar chart: Model performance comparison\n",
    "- Gauge chart: Overall forecast accuracy\n",
    "\n",
    "### Page 2: Forecast Analysis  \n",
    "- Time series: Multiple model comparisons\n",
    "- Scatter plot: Actual vs Predicted values\n",
    "- Error analysis: Daily error rates\n",
    "- Model selection: Interactive model comparison\n",
    "\n",
    "### Page 3: Business Intelligence\n",
    "- Seasonal heatmap: Monthly patterns\n",
    "- Trend analysis: Year-over-year growth\n",
    "- Future projections: Next 30 days forecast\n",
    "- Business insights: Peak/low periods identification\n",
    "\n",
    "## Key Metrics to Highlight:\n",
    "- ARIMA Model: 7.06% MAPE (Best Performance)\n",
    "- Linear Regression: 7.52% MAPE  \n",
    "- Average Daily Sales: $637K\n",
    "- Forecast Period: 90 days (Apr-Aug 2017)\n",
    "\"\"\"\n",
    "\n",
    "with open('output/gds_setup_guide.txt', 'w') as f:\n",
    "    f.write(gds_guide)\n",
    "\n",
    "print(\"\\nGoogle Data Studio datasets exported successfully:\")\n",
    "print(\"- gds_main_forecasting.csv (forecast comparisons)\")\n",
    "print(\"- gds_model_performance.csv (model metrics)\")\n",
    "print(\"- gds_historical_analysis.csv (historical data)\")\n",
    "print(\"- gds_seasonal_insights.csv (seasonal patterns)\")\n",
    "print(\"- gds_weekly_accuracy.csv (accuracy analysis)\")\n",
    "print(\"- gds_future_forecasts.csv (future projections)\")\n",
    "print(\"- gds_business_summary.csv (executive KPIs)\")\n",
    "print(\"- gds_setup_guide.txt (dashboard setup guide)\")\n",
    "\n",
    "print(f\"\\nKey Insights for Dashboard:\")\n",
    "print(f\"• Best Model: ARIMA with {gds_main['arima_pct_error'].mean():.2f}% average error\")\n",
    "print(f\"• Total Forecast Period: {len(gds_main)} days\")\n",
    "print(f\"• Average Daily Sales: ${gds_main['actual'].mean():,.0f}\")\n",
    "print(f\"• Seasonal Peak: December (${seasonal_insights[seasonal_insights['month']==12]['mean_sales'].iloc[0]:,.0f} avg)\")\n",
    "print(f\"• Future Revenue Projection: ${future_forecast_df['forecast_sales'].sum():,.0f} (next 30 days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
